# -*- coding: utf-8 -*-
# """DatosMaestro_EPM_Corr.ipynb

# Automatically generated by Colab.

# Original file is located at
#     https://colab.research.google.com/drive/1zYTMeugmpitYD-6YdjqwsMA0pbmoNjTf
# """

#!pip install tensorflow

# Importar las bibliotecas necesarias
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GRU, Dense, InputLayer
import matplotlib.pyplot as plt

# Montar Google Drive (asumiendo que los datos están allí)
from google.colab import drive
drive.mount('/content/drive')

# Cargar los datos
file_path = '/content/drive/MyDrive/DIPLOMADO EPM/Datos/BD_maestro.csv'
datos_Maestro = pd.read_csv(file_path)
datos_Maestro['fecha'] = pd.to_datetime(datos_Maestro['fecha'])
datos_Maestro.set_index('fecha', inplace=True)

# Preparación de datos
features = datos_Maestro.drop(['ENSA', 'dummy'], axis=1)  # Excluir columnas no necesarias
labels = datos_Maestro['ENSA']

# Escalar las características
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)
features_scaled_df = pd.DataFrame(features_scaled, columns=features.columns, index=features.index)

datos_Maestro.describe()

# Calcular la matriz de correlación
correlation_matrix = datos_Maestro.corr()

# Considerar solo las correlaciones con respecto a 'ENSA'
correlation_with_ENSA = correlation_matrix['ENSA'].sort_values(ascending=False)

# Seleccionar características con una correlación significativa

selected_features = correlation_with_ENSA[abs(correlation_with_ENSA) > 0.8].index.tolist()
selected_features.remove('ENSA')  # Asegurarse de no incluir la variable objetivo

# Revisar las características seleccionadas
print("Características seleccionadas basadas en correlación:", selected_features)

# Preparar datos con las características seleccionadas
features_selected = datos_Maestro[selected_features]

# Escalar las características seleccionadas
scaler = StandardScaler()
features_selected_scaled = scaler.fit_transform(features_selected)
features_selected_scaled_df = pd.DataFrame(features_selected_scaled, columns=selected_features)

# Función para crear secuencias temporales
def create_sequences(features, labels, window_size):
    X, y = [], []
    for i in range(len(features) - window_size):
        X.append(features.iloc[i:(i + window_size)].values)
        y.append(labels.iloc[i + window_size])
    return np.array(X), np.array(y)

# Crear secuencias
window_size = 6
X, y = create_sequences(features_selected_scaled_df, datos_Maestro['ENSA'], window_size=12)

# Dividir los datos en entrenamiento y validación
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Definir la arquitectura del modelo
model = Sequential([
    InputLayer(input_shape=(X_train.shape[1], X_train.shape[2])),
    GRU(50, activation='relu', return_sequences=True),
    GRU(50, activation='tanh'),
    Dense(20, activation='linear'),
    Dense(1)
])

# Compilar y entrenar el modelo
model.compile(optimizer='adam', loss='mean_squared_error')
history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_val, y_val))

# Graficar la historia de entrenamiento
plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='validation')
plt.legend()
plt.show()

# Preparar los datos para la predicción futura
if len(X_val) >= 6:
    # Tomar los últimos 12 registros de X_val
    last_window = X_val[-6:]  # Asegúrate de que estás tomando los últimos 12 timesteps

    # Verificación antes de expand_dims para asegurar la correcta manipulación de los datos
    print("Verificación de forma de last_window antes de expand_dims:", last_window.shape)

    # last_window ya debería tener la forma (12, 8) y sólo necesitas ajustarlo para que sea un batch
    if last_window.shape == (12, 8):
        last_window = np.expand_dims(last_window, axis=0)
        print("Forma de last_window después de expand_dims:", last_window.shape)

        # Realizar la predicción con el modelo
        predictions = model.predict(last_window)
        print("Predicciones:", predictions)
    else:
        print("La forma de last_window no es correcta, revisa la preparación de los datos.")
else:
    print("No hay suficientes datos en X_val para crear 'last_window' para predicciones")

# Generar fechas futuras
last_known_date = datos_Maestro.index.max()
future_dates = pd.date_range(start=last_known_date + pd.DateOffset(months=1), periods=6, freq='MS')

# generar predicciones
predictions = model.predict(last_window)

# Asegúrate de que las predicciones tienen la forma correcta
predictions = predictions.flatten()  # Aplanar el array si es necesario

# Crear un DataFrame para las predicciones
predicted_data = pd.DataFrame({
    'fecha': future_dates,
    'ENSA_pred': predictions
})

# Establecer 'fecha' como índice si prefieres trabajar con índices de fecha
predicted_data.set_index('fecha', inplace=True)

# Graficar los resultados
plt.figure(figsize=(32, 12))
plt.plot(datos_Maestro.index, datos_Maestro['ENSA'], label='Historical ENSA', color='blue')
plt.plot(predicted_data.index, predicted_data['ENSA_pred'], label='Predicted ENSA for 2024', color='red', linestyle='--')
plt.title('Historical and Predicted ENSA from 2010 to mid-2024')
plt.xlabel('Date')
plt.ylabel('ENSA')
plt.legend()
plt.grid(True)
plt.show()